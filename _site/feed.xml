<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-04-30T16:19:04+09:00</updated><id>http://localhost:4000/</id><title type="html">kaushalya.github.io</title><subtitle>A blog about technology and stuff related</subtitle><entry><title type="html">Reading list - Compression of Deep Neural Networks</title><link href="http://localhost:4000/reading-list-Deep-compression/" rel="alternate" type="text/html" title="Reading list - Compression of Deep Neural Networks" /><published>2018-04-13T00:00:00+09:00</published><updated>2018-04-13T00:00:00+09:00</updated><id>http://localhost:4000/reading-list-Deep-compression</id><content type="html" xml:base="http://localhost:4000/reading-list-Deep-compression/">&lt;p&gt;Here is a list some of the papers I had read as literature review for the &lt;a href=&quot;https://www.jst.go.jp/kisoken/crest/en/project/1111094/1111094_07.html&quot;&gt;“CREST Deep”&lt;/a&gt; project. This project is funded by Japan Science and Technology Agency (JST). Our goal is to make the DNN models smaller, so they take less disk space, but without having a significant impact on the accuracy.&lt;/p&gt;

&lt;p class=&quot;notice&quot;&gt;&lt;a href=&quot;https://papers.nips.cc/paper/6687-compression-aware-training-of-deep-networks&quot;&gt;&lt;strong&gt;Compression-aware Training of Deep Networks&lt;/strong&gt;&lt;/a&gt;. Alvarez and Salzmann. NIPS 2017&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;This paper introduces a regularization term which encourages the parameter matrix of each layer to have low rank while training, thus improving low rank decomposition based compression techniques. Since explicitly minimizing the rank of a matrix is NP-hard, regularization term uses nuclear norm instead. Where, the nuclear norm is defined as summation of singular values of parameter matrix of a layer. In addition, group Lasso regularization term is introduced to reduce the number pf parameters further. Experiments of this paper are limited to &lt;em&gt;DecomposeMe&lt;/em&gt; network architecture, in which each convolutional layer is decomposed into two 1D kernels. Even though it is claimed that a decomposed ResNet-50 network is also used, the results are not reported in the paper.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class=&quot;notice&quot;&gt;&lt;strong&gt;Do Deep Convolutional Nets Really Need to be Deep and Convolutional?&lt;/strong&gt; Gregor Urban et al. ICLR 2017&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;This can be considered as a response to the NIPS 2014 paper “Do deep nets really need to be deep?” (Ba and Caruana, 2014). They train shallowe Colnvolutional Neural Networks using network distillation on an ensemble of state-of-the-art CNNs on CIFAR-10 dataset. Their results suggest that to achieve similar accuracy with a shallow student model, it should possess much more parameters than the deep teacher model (can be 30 times larger than the deep teacher model). Still the accuracy may not reach the level of the teacher model.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class=&quot;notice&quot;&gt;&lt;strong&gt;MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications&lt;/strong&gt;. Howard et al. (Google Inc.) Arxiv preprint [27th April 2017]&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;This paper introduces a smaller and faster convolutional neural network achitecture by replacing convolutions by depthwise separable convolutions and a 1x1 pointwise convolution to combine the outputs of depthwise convolutions. Additionally this paper introduces two more parameters, width multiplier and resolution multiplier to reduce the number of hyperparameters further.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class=&quot;notice&quot;&gt;&lt;strong&gt;Dynamic Network Surgery for Efficient DNNs&lt;/strong&gt;. Guo et al., NIPS. 2016&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;This paper was &lt;a href=&quot;https://papers.nips.cc/paper/6165-dynamic-network-surgery-for-efficient-dnns&quot;&gt;presented&lt;/a&gt; as a poster at NIPS 2016. Two operations: pruning and splicing (recovery of pruned connections) are performed in a continuous manner. Connections are pruned based on the magnitude of their weights. Authors claim a 17.7X compression rate for AlexNet.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class=&quot;notice&quot;&gt;&lt;strong&gt;Learning Structured Sparsity in Deep Neural Networks&lt;/strong&gt;. Wen et al., NIPS. 2016&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;This paper was &lt;a href=&quot;http://papers.nips.cc/paper/6504-learning-structured-sparsity-in-deep-neural-networks&quot;&gt;presented&lt;/a&gt; a poster at NIPS 2016. This paper introduces group lasso based sparsity regularization to zero out all weights in some structures (filters, channels, and layers) without a significant drop of classification accuracy.&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>Kaushalya</name></author><category term="blog" /><summary type="html">Deep neural network compression literature research organized chronologically.</summary></entry></feed>